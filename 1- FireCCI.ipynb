{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c8363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Authenticate(auth_mode='notebook')\n",
    "ee.Initialize(project='global-booster-421311')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b8ccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2002...\n",
      "Export started for 2002 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2003...\n",
      "Export started for 2003 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2004...\n",
      "Export started for 2004 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2005...\n",
      "Export started for 2005 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2006...\n",
      "Export started for 2006 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2007...\n",
      "Export started for 2007 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2008...\n",
      "Export started for 2008 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2009...\n",
      "Export started for 2009 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2010...\n",
      "Export started for 2010 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2011...\n",
      "Export started for 2011 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2012...\n",
      "Export started for 2012 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2013...\n",
      "Export started for 2013 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2014...\n",
      "Export started for 2014 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2015...\n",
      "Export started for 2015 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2016...\n",
      "Export started for 2016 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2017...\n",
      "Export started for 2017 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2018...\n",
      "Export started for 2018 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "Processing year 2019...\n",
      "Export started for 2019 â€“ Monitor in GEE Tasks or Google Drive.\n",
      "All exports started!\n"
     ]
    }
   ],
   "source": [
    "catchments = ee.FeatureCollection(\"projects/global-booster-421311/assets/LakeCatchments_NA\")\n",
    "landcover_codes = [0, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 170, 180]\n",
    "export_fields = ['Lake_ID', 'year', 'week'] + [f'burned_pix_lc_{lc}' for lc in landcover_codes]\n",
    "\n",
    "# Specify your years here\n",
    "years = list(range(2002, 2020))  # 2002 to 2020\n",
    "\n",
    "def features_for_week(image, year, week):\n",
    "    week = ee.Number(week)\n",
    "    start_day = week.subtract(1).multiply(7).add(1)\n",
    "    end_day = week.multiply(7).min(366)\n",
    "    burned = image.select('BurnDate').gte(start_day).And(image.select('BurnDate').lte(end_day))\n",
    "    landcover = image.select('LandCover')\n",
    "    bands = [burned.And(landcover.eq(lc)).rename(f'burned_pix_lc_{lc}') for lc in landcover_codes]\n",
    "    stack = ee.Image.cat(bands)\n",
    "    stats = stack.reduceRegions(\n",
    "        collection=catchments,\n",
    "        reducer=ee.Reducer.sum().forEachBand(stack),\n",
    "        scale=250\n",
    "    ).map(lambda f: f.set({'year': year, 'week': week}))\n",
    "    return stats\n",
    "\n",
    "def drop_geometry(feature):\n",
    "    return feature.setGeometry(None)\n",
    "\n",
    "for year in years:\n",
    "    print(f\"Processing year {year}...\")\n",
    "    image = ee.ImageCollection('ESA/CCI/FireCCI/5_1').filterDate(f'{year}-01-01', f'{year+1}-01-01').first()\n",
    "    if image is None:\n",
    "        print(f\"No FireCCI image for year {year}, skipping...\")\n",
    "        continue\n",
    "    weeks = list(range(1, 54))  # 1 to 53\n",
    "    # Map over all weeks for this year\n",
    "    weekly_collections = ee.List(weeks).map(lambda w: features_for_week(image, year, ee.Number(w)))\n",
    "    all_features = ee.FeatureCollection(weekly_collections).flatten()\n",
    "    all_features_no_geom = all_features.map(drop_geometry)\n",
    "\n",
    "    task = ee.batch.Export.table.toDrive(\n",
    "        collection=all_features_no_geom.select(export_fields),\n",
    "        description=f'FireCCI51_BurnedPixels_Catchments_Weekly_{year}',\n",
    "        fileFormat='CSV'\n",
    "    )\n",
    "    task.start()\n",
    "    print(f\"Export started for {year} â€“ Monitor in GEE Tasks or Google Drive.\")\n",
    "\n",
    "print(\"All exports started!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbb6b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2002.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2003.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2004.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2005.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2006.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2007.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2008.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2009.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2010.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2011.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2012.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2013.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2014.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2015.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2016.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2017.csv\n",
      "âœ… Processed FireCCI51_BurnedPixels_Catchments_Weekly_2018.csv\n",
      "ðŸŽ‰ All weekly FireCCI tables updated with descriptive names and dates!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "def week_start_end(year, week):\n",
    "    start = datetime(year, 1, 1) + timedelta(days=(week - 1) * 7)\n",
    "    end = min(start + timedelta(days=6), datetime(year, 12, 31))\n",
    "    return start.strftime('%Y/%m/%d'), end.strftime('%Y/%m/%d')\n",
    "\n",
    "landcover_code_to_name = {\n",
    "    0: \"Cropland_rainfed\",\n",
    "    20: \"Cropland_irrigated\",\n",
    "    30: \"Mosaic_cropland\",\n",
    "    40: \"Mosaic_natural_veg\",\n",
    "    50: \"Tree_broad_evergreen\",\n",
    "    60: \"Tree_broad_deciduous\",\n",
    "    70: \"Tree_needle_evergreen\",\n",
    "    80: \"Tree_needle_deciduous\",\n",
    "    90: \"Tree_mixed\",\n",
    "    100: \"Mosaic_tree_shrub\",\n",
    "    110: \"Mosaic_herbaceous\",\n",
    "    120: \"Shrubland\",\n",
    "    130: \"Grassland\",\n",
    "    140: \"Lichens_mosses\",\n",
    "    150: \"Sparse_vegetation\",\n",
    "    170: \"Tree_flooded_saline\",\n",
    "    180: \"Shrub_herb_flooded\"\n",
    "}\n",
    "\n",
    "folder = \"Datasets/firecci_raw\"  # Update if your CSVs are in a different folder\n",
    "\n",
    "# Loop over all CSV files in the folder\n",
    "for fname in os.listdir(folder):\n",
    "    if fname.endswith('.csv') and fname.startswith('FireCCI51_BurnedPixels_Catchments_Weekly'):\n",
    "        fpath = os.path.join(folder, fname)\n",
    "        df = pd.read_csv(fpath)\n",
    "\n",
    "        # Drop unwanted columns if present\n",
    "        to_drop = ['system:index', '.geo']\n",
    "        df = df.drop(columns=[c for c in to_drop if c in df.columns])\n",
    "\n",
    "        # Add week_start and week_end columns\n",
    "        df[['week_start', 'week_end']] = df.apply(\n",
    "            lambda row: week_start_end(int(row['year']), int(row['week'])), axis=1, result_type='expand'\n",
    "        )\n",
    "\n",
    "        # Rename land cover columns to descriptive names\n",
    "        rename_dict = {\n",
    "            f\"burned_pix_lc_{code}\": f\"{name}\"\n",
    "            for code, name in landcover_code_to_name.items()\n",
    "        }\n",
    "        df = df.rename(columns=rename_dict)\n",
    "\n",
    "        # Reorder columns\n",
    "        main_cols = ['Lake_ID', 'year', 'week', 'week_start', 'week_end']\n",
    "        other_cols = [col for col in df.columns if col not in main_cols]\n",
    "        df = df[main_cols + other_cols]\n",
    "\n",
    "        # Sort by Lake_ID, year, week\n",
    "        df = df.sort_values(by=['Lake_ID', 'year', 'week'])\n",
    "\n",
    "        # Save (overwrite or save as new)\n",
    "        df.to_csv(fpath, index=False)\n",
    "        print(f\"âœ… Processed {fname}\")\n",
    "\n",
    "print(\"ðŸŽ‰ All weekly FireCCI tables updated with descriptive names and dates!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575656e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                LandCover          LandCoverName  TotalBurnedPixels\n",
      "0               Grassland              Grassland       28249.882353\n",
      "1    Tree_broad_deciduous   Tree_broad_deciduous       23558.815686\n",
      "2      Shrub_herb_flooded     Shrub_herb_flooded       17873.615686\n",
      "3               Shrubland              Shrubland       16407.917647\n",
      "4   Tree_needle_evergreen  Tree_needle_evergreen       13591.498039\n",
      "5      Mosaic_natural_veg     Mosaic_natural_veg        8187.513725\n",
      "6              Tree_mixed             Tree_mixed        4983.282353\n",
      "7       Mosaic_herbaceous      Mosaic_herbaceous        3896.486275\n",
      "8         Mosaic_cropland        Mosaic_cropland        3877.368627\n",
      "9    Tree_broad_evergreen   Tree_broad_evergreen        2554.760784\n",
      "10      Mosaic_tree_shrub      Mosaic_tree_shrub        1870.474510\n",
      "11  Tree_needle_deciduous  Tree_needle_deciduous        1557.098039\n",
      "12    Tree_flooded_saline    Tree_flooded_saline         512.941176\n",
      "13      Sparse_vegetation      Sparse_vegetation          24.000000\n",
      "14       Cropland_rainfed       Cropland_rainfed           0.000000\n",
      "15         Lichens_mosses         Lichens_mosses           0.000000\n",
      "16     Cropland_irrigated     Cropland_irrigated           0.000000\n",
      "\n",
      "Top 5 most affected land covers (all years):\n",
      "           LandCoverName  TotalBurnedPixels\n",
      "0              Grassland       28249.882353\n",
      "1   Tree_broad_deciduous       23558.815686\n",
      "2     Shrub_herb_flooded       17873.615686\n",
      "3              Shrubland       16407.917647\n",
      "4  Tree_needle_evergreen       13591.498039\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = \"Datasets/firecci_raw\"  # update if needed\n",
    "\n",
    "# Find all FireCCI CSVs in the folder\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if f.startswith('FireCCI51_BurnedPixels_Catchments_Weekly') and f.endswith('.csv')]\n",
    "\n",
    "# Concatenate all years\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Identify land cover columns\n",
    "meta_cols = ['Lake_ID', 'year', 'week', 'week_start', 'week_end']\n",
    "landcover_cols = [col for col in df_all.columns if col not in meta_cols]\n",
    "\n",
    "# Aggregate burned pixels by land cover\n",
    "total_burned_by_lc = df_all[landcover_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "# Optionally map codes to names\n",
    "landcover_code_to_name = {\n",
    "    0: \"Cropland_rainfed\", 20: \"Cropland_irrigated\", 30: \"Mosaic_cropland\",\n",
    "    40: \"Mosaic_natural_veg\", 50: \"Tree_broad_evergreen\", 60: \"Tree_broad_deciduous\",\n",
    "    70: \"Tree_needle_evergreen\", 80: \"Tree_needle_deciduous\", 90: \"Tree_mixed\",\n",
    "    100: \"Mosaic_tree_shrub\", 110: \"Mosaic_herbaceous\", 120: \"Shrubland\",\n",
    "    130: \"Grassland\", 140: \"Lichens_mosses\", 150: \"Sparse_vegetation\",\n",
    "    170: \"Tree_flooded_saline\", 180: \"Shrub_herb_flooded\"\n",
    "}\n",
    "def get_name(col):\n",
    "    for code, name in landcover_code_to_name.items():\n",
    "        if str(code) in col:\n",
    "            return name\n",
    "    return col\n",
    "\n",
    "summary_df = total_burned_by_lc.reset_index()\n",
    "summary_df.columns = ['LandCover', 'TotalBurnedPixels']\n",
    "summary_df['LandCoverName'] = summary_df['LandCover'].apply(get_name)\n",
    "\n",
    "print(summary_df[['LandCover', 'LandCoverName', 'TotalBurnedPixels']])\n",
    "\n",
    "print(\"\\nTop 5 most affected land covers (all years):\")\n",
    "print(summary_df[['LandCoverName', 'TotalBurnedPixels']].head(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
